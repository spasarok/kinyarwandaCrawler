Kim Spasaro
August 14, 2013
github

--UNDER CONSTRUCTION--

Directions for installing and running Nutch (1.7) with Solr (3.6.2) integration.

Note: Nutch (2.2.1) and Solr (4.4.0) are the currently the most recent
   Nutch/Solr releases. Because Nutch (2.x) is significantly less-documented than 
   the (1.x) series, and because the (2.x) series is significant overhaul to the 
   (1.x) series, I opted to stick with the tried and true Nutch (1.7) and Solr 
   (3.6.2). These directions do NOT apply to Nutch (2.x) or Solr (4.x) and will 
   NOT work for new releases.

0. Links
   
   [1] Current Solr Release
       http://lucene.apache.org/solr/
    
   [2] Past Solr Releases
       http://archive.apache.org/dist/lucene/solr/
   
   [3] Current Nutch Release
       http://nutch.apache.org/downloads.html
   
   [4] Past Nutch Releases
       http://archive.apache.org/dist/nutch/

   [5] Official Apache Nutch Tutorial
       http://wiki.apache.org/nutch/NutchTutorial
   
1. Install Solr (3.6.2)

   Download the Solr (3.6.2) binary from link [2].  
   **Do NOT install a Solr source distribution (anything containing .src in 
   the download name)--I am NOT going to tell you how to compile Solr from 
   source. You'll probably want to download <apache-solr-3.6.2.tgz> or 
   <apache-solr-3.6.2.zip> for this tutorial.

   Solr Reference:

      * working Solr example            | SOLR_HOME/example
      * configuration files             | SOLR_HOME/example/solr/conf
      * small Jetty package             | SOLR_HOME/example/lib
      * <schema.xml> ~ | example schema | SOLR_HOME/example/solr/conf/schema.xml
      * <solr.log>     | runtime logs   | SOLR_HOME/example/logs/solr.log
      * <solr.war>     | Solr WAR       | SOLR_HOME/example/webapps/solr.war
      * <start.jar>    | start JAR      | SOLR_HOME/example/start.jar

        ~  file generated during crawl

2. Install Nutch (1.7)

   Download the Nutch (1.7) binary from link [4]. 
   **Do NOT install a Nutch source distribution (anything containing .src in 
   the download name)--I am NOT going to tell you how to compile Nutch from 
   source.  You'll probably want to download <apache-nutch-1.7-bin.tar.gz> or 
   <apache-nutch-1.7-bin.zip>.

   Nutch Reference:
      
      * crawl command                               | NUTCH_HOME/bin/crawl
      * configuration files                         | NUTCH_HOME/conf
      * <nutch-default.xml>    | default properties | NUTCH_HOME/conf/nutch-default.xml
      * <nutch-site.xml>       | property overrides | NUTCH_HOME/conf/nutch-site.xml
      * <regex-urlfilter.xml>  | crawl scope        | NUTCH_HOME/conf/regex-urlfilter.xml
      * <hadoop.log> ~         | runtime logs       | NUTCH_HOME/logs/hadoop.log
      * <seed.txt> ~~          | seed URLS          | NUTCH_HOME/urls/seed.txt
        
        ~  file generated during crawl
        ~~ file and path generated by user

3. Set JAVA_HOME

   Blah

4. Configure Solr

   <SOLR_HOME/example> provides a working example directory. We'll use
   this to run Nutch.

   An example Solr schema <schema.xml> is contained at <SOLR_HOME/example/solr/conf/schema.xml>.
   This is NOT the schema we want to use with Nutch. The correct schema is 
   contained at <NUTCH_HOME/conf/schema.xml>. Replace Solr's example schema 
   with the schema provided by Nutch.

5. Configure Nutch

   Nutch configuration files are contained in <NUTCH_HOME/conf>. Important 
   configuration files are:

      * <nutch-default.xml>    | default crawl properties
      * <nutch-site.xml>       | property overrides
      * <regex-urlfilter.xml>  | information about crawl scope

   <nutch-default.xml>

      This file contains the default crawl properties. Do NOT change any of the 
      properties in this file.  They are for reference only.

   <nutch-site.xml>
   
      This file contains overrides of default crawl properties. By default
      this file is empty. Place property overrides in this file by copying
      and modifying properties given in <nutch-default.xml>. Note that the
      <http.agent.name> property MUST be set in <nutch-site.xml>. An example
      <nutch-site.xml> is contained in the <examples> folder of this tutorial.

   <regex-urlfilter.xml>

      This file uses regular expressions to direct or restrict crawl scope.
      <+> indicates domains to include in the crawl, while <-> indicates domains
      to exclude from the crawl. For example, if I want to crawl the Apache
      Nutch domain but I want to exclude the Apache Nutch Wiki I would use the
      regular expressions:

         +^http://([a-z0-9]*\.)*nutch.apache.org/
         -^http://([a-z0-9]*\.)*nutch.apache.org/wiki
 
      An example <regex-urlfilter.xml> is contained in the examples folder of 
      this tutorial.

   Seed URLS    

      After configuring the crawl you need to create a list of seed URLS. These
      URLS will be used as the starting points for the crawl. While you can place 
      the URL seeds wherever you choose (as long as you correctly indicate their
      location when calling the crawl command), convention is to create a <urls> 
      folder in <NUTCH_HOME> and store the seed URLS at <NUTCH_HOME/urls.seed.txt>. 
      In <seed.txt> list the URLs of the domains you wish to crawl, one URL per 
      line. An example <seed.txt> is contained in the examples folder of this 
      tutorial.

   Crawl Command
  
      The crawl command is located at <NUTCH_HOME/bin/crawl>. This file contains
      the script you will call to run the crawl. You can change script param-
      eters to suit the needs of your crawl. Parameters you'll probably want to
      modify are:

         # number of urls to fetch in one iteration
         sizeFetchlist=`expr $numSlaves \* 50000`

         # time limit for feching
         timeLimitFetch=180

         # topN: num threads for fetching
         numThreads=50

6. Start Solr

   From the command line, navigate to <SOLR_HOME/example>. This folder contains
   the JAR necessary to start Solr.  Start Solr with the command:

      SOLR_HOME/example$ java -jar start.jar

   Check that Solr is running by navigating to <localhost:8983/solr/admin>
   in your browser. If Solr is running, this will bring you to the Solr query
   interface.

7. Run the Crawl

   Parameters for the crawl command are:

      <seedURLs>       | location of the .txt containing seed URLS
      <crawlID>        | directory containing crawl contents (generated during crawl)
      <solrURL>        | localhost:8983/solr/
      <numberOfRounds> | number of times to repeat the crawl

   Run the crawl command (from command line) with:
      NUTCH_HOME$ bin/crawl <seedURLs> <crawlID> <solrURL> <numberOfRounds>

   For example,
      NUTCH_HOME$ bin/crawl urls/seed.txt TestCrawl localhost:8983/solr/ 2

   Nutch will place crawl contents in <NUTCH_HOME/crawlID>
   Solr will place crawl contents in <SOLR_HOME/example/data>

8. Logs and Errors

   Both Solr and Nutch produce crawl logs. You'll want to take a good look at
   these if your crawl produces errors. Nutch logs are located at
   <NUTCH_HOME/logs/hadoop.log>. Solr logs are located at
   <SOLR_HOME/example/logs/solr.log>.  Note that these logs are not overwritten
   by successive crawls, so it's important to pay attention to log timestamps.

9. View Output

   Make sure Solr is running and navigate to <localhost:8983/solr/admin>
   in your browser. Querying <*|*> should generate, in XML, a list of all data 
   indexed by the crawler. 

   By default Solr lists the first 10 query matches. You can change this by 
   modifying the parameters of the query's result URL. You'll want to change the
   <start> and <rows> parameters to adjust the number of query results you view.
   <start> designates at which match the query results will begin. <rows>
   designates the number of matches the query results will display. For example,
   setting <start=0> and <rows=10> will show the first 10 query matches, while
   changing the parameters to <start=30> and <rows=20> will show query matches
   31-50.

   The <numFound> attribute of the <results> tag indicates how many crawl indices 
   match the query. For example, if I query <*|*> (which matches all crawl results)
   and the XML result indicates <numFound=100> I know that my crawl indexed
   100 sites.

10. Running a New Crawl

    To run a new crawl, just delete, move, or rename the directories containing 
    Nutch and Solr's crawl results. Note that while deleting <NUTCH_HOME/crawlID> 
    will remove all information Nutch stored for the crawl, simply running a new 
    crawl will NOT remove past Solr indices--and querying Solr will show old 
    crawl data. To reset Solr's indices you must also delete, move, or rename 
    <SOLR_HOME/example/data>.

